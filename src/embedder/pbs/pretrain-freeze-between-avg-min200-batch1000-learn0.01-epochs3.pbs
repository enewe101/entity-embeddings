#!/bin/bash
#PBS -l nodes=1:ppn=12:gpus=1
#PBS -l walltime=12:00:00
#PBS -o /gs/project/eeg-641-aa/enewel3/entity-embeddings/src/embedder/pbs/pretrain-freeze-between-avg-min200-batch1000-learn0.01-epochs3.stdout
#PBS -e /gs/project/eeg-641-aa/enewel3/entity-embeddings/src/embedder/pbs/pretrain-freeze-between-avg-min200-batch1000-learn0.01-epochs3.stderr
#PBS -N pretrain-freeze-between-avg-min200-batch1000-learn0.01-epochs3

# Load modules, go to source dir, activate virtualenv
module load Python/2.7.10 CUDA/7.5.18 cuDNN/5.0-ga
cd /gs/project/eeg-641-aa/enewel3/entity-embeddings/src/embedder 
source ../../env/bin/activate

# Define variables
save_dir=/gs/project/eeg-641-aa/enewel3/entity-embeddings/data/pretrain-freeze-between-avg-min200-batch1000-learn0.01-epochs3
context_embeddings_fname=/gs/project/eeg-641-aa/enewel3/entity-embeddings/data/google-vectors-negative-300.txt
load_dictionary_dir=/gs/project/eeg-641-aa/enewel3/entity-embeddings/data/dictionaries

# Run the model
THEANO_FLAGS='floatX=float32,device=gpu' python train_r2v.py \
command=train \
save_dir=$save_dir \
load_dictionary_dir=$load_dictionary_dir \
context_embeddings_fname=$context_embeddings_fname \
batch_size=1000 \
macrobatch_size=50000 \
noise_ratio=15 \
min_query_frequency=20 \
min_context_frequency=20 \
min_entity_pair_frequency=200 \
num_embedding_dimensions=300 \
num_epochs=3 \
learning_rate=0.01 \
num_processes=12 \
momentum=0.9 \
max_queue_size=2 \
verbose=true \
read_data_async=true \
freeze_context=true \
entity_noise_ratio=0.5 \
signal_sample_mode=between \
len_context=6

# python sanity_check.py embeddings_dir=$save_dir

